{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your text file\n",
        "file_path = '/content/drive/MyDrive/CBOW.txt'\n",
        "\n",
        "# Open the file in read mode\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the entire contents of the file\n",
        "    file_contents = file.read()\n"
      ],
      "metadata": {
        "id": "eH6GzRFw0f-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents"
      ],
      "metadata": {
        "id": "nJnjPQ7WTuX-",
        "outputId": "3ac3cd3f-34d1-454a-9d01-227e9a8b9335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \\n\\nFurther, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \\n\\nThe reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k7kxBn1uGm1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample sentences\n",
        "sentences = file_contents.split('.')\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Generate context-target pairs for training\n",
        "window_size = 3\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "print(tokenized_sentences)\n",
        "\n",
        "data, labels = [], []\n",
        "for sentence in tokenized_sentences:\n",
        "    for i, target_word in enumerate(sentence):\n",
        "        context = [\n",
        "            sentence[j] for j in range(i - window_size, i + window_size + 1)\n",
        "            if j != i and 0 <= j < len(sentence)\n",
        "        ]\n",
        "        data.append(context)\n",
        "        labels.append(target_word)\n",
        "\n",
        "# Convert data and labels to numpy arrays\n",
        "data = pad_sequences(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Define CBOW model\n",
        "embedding_dim = 50\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=window_size * 2),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(data, labels, epochs=200, verbose=1)\n",
        "\n",
        "# Get the word embeddings\n",
        "word_embeddings = model.get_layer(index=0).get_weights()[0]\n",
        "\n",
        "# Save or use the word embeddings as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos(a,b):\n",
        "  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ],
      "metadata": {
        "id": "Cx7x6VHb6RHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(tokenizer.word_index.keys())"
      ],
      "metadata": {
        "id": "psYkFNM8-xAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "target_word = 'influenza'\n",
        "\n",
        "\n",
        "target_word_index = tokenizer.word_index[target_word]\n",
        "\n",
        "\n",
        "target_embedding = word_embeddings[target_word_index]\n",
        "\n",
        "\n",
        "similarities = cosine_similarity(target_embedding.reshape(1, -1), word_embeddings)[0]\n",
        "\n",
        "\n",
        "most_similar_indices = similarities.argsort()[-5:][::-1]  # Adjust the number of similar words as needed\n",
        "\n",
        "\n",
        "most_similar_words = [word for word, idx in tokenizer.word_index.items() if idx in most_similar_indices]\n",
        "\n",
        "print(f\"Most similar words to '{target_word}': {most_similar_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cxkrwCy-8mu",
        "outputId": "784cd576-c143-4d6a-eccf-466f6c5dc9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'influenza': ['influenza', 'means', 'spread', 'learning', 'people']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path =''\n",
        "with open(file_path, 'r') as file:\n",
        "  file_contents= file.read()"
      ],
      "metadata": {
        "id": "TEclxY0G4dOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, model\n",
        "from tensorflow.keras.preprocessing.texts import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequences import pad_sequences\n",
        "\n",
        "\n",
        "sentences=file_contents.split('.')\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "\n",
        "tokenized_sentences=tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "window_size=3\n",
        "data, labels = [], []\n",
        "for i in tokenized_sentences:\n",
        "  for j, target_word in enumerate(i):\n",
        "    context = [sentence[k] for k in range(j-window_size, j+window_size+1) and k!=j and 0<=k<len(i)]\n",
        "  data.append(context)\n",
        "  labels.append(target_word)\n",
        "\n",
        "data= pad_sequences(data)\n",
        "labels=np.array(labels)\n",
        "emb_size=50\n",
        "model=models.Sequential([\n",
        "    layers.Embedding(input_dim= total_words, output_dim=emb_size, input_length=2*window_size )\n",
        "    layers.GlobalAveragePooling1D()\n",
        "    layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(data, labels, epochs=200)\n",
        "\n",
        "word_embeddings=model.get_layer(index=0).get_weights()[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DWyLLf9GMIVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}