{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258c5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Dense,GlobalAveragePooling1D\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d04bc",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2db6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19.',\n",
       " '',\n",
       " 'Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission.',\n",
       " '',\n",
       " 'The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus=[\"The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.\"]\n",
    "\n",
    "corpus = [] #The corpus is a collection of text documents, and each document is a sequence of words.\n",
    "with open(\"../LP-IV-datasets/CBOW/CBOW.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        corpus.append(line.strip()) #reads each line from the text.txt file, removes any whitespace from the end of the \n",
    "        \t\t\t\t\t\t\t#line, and adds the line to the corpus list.\n",
    "            \n",
    "corpus\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b439e3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  38,\n",
       "  2,\n",
       "  8,\n",
       "  9,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  2,\n",
       "  42,\n",
       "  13,\n",
       "  1,\n",
       "  43,\n",
       "  23,\n",
       "  3,\n",
       "  44,\n",
       "  11,\n",
       "  24,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  1,\n",
       "  14,\n",
       "  25,\n",
       "  48,\n",
       "  10,\n",
       "  26,\n",
       "  2,\n",
       "  27,\n",
       "  12,\n",
       "  11,\n",
       "  24,\n",
       "  15,\n",
       "  16,\n",
       "  1,\n",
       "  14,\n",
       "  13,\n",
       "  49,\n",
       "  50,\n",
       "  17,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  15,\n",
       "  16,\n",
       "  7,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  51,\n",
       "  10,\n",
       "  18,\n",
       "  19,\n",
       "  52,\n",
       "  20,\n",
       "  28,\n",
       "  7,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  15,\n",
       "  16,\n",
       "  9,\n",
       "  29,\n",
       "  20,\n",
       "  30,\n",
       "  53,\n",
       "  31,\n",
       "  3,\n",
       "  32,\n",
       "  54,\n",
       "  55,\n",
       "  17,\n",
       "  4,\n",
       "  5],\n",
       " [],\n",
       " [56,\n",
       "  8,\n",
       "  33,\n",
       "  1,\n",
       "  57,\n",
       "  29,\n",
       "  19,\n",
       "  20,\n",
       "  2,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  8,\n",
       "  63,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  64,\n",
       "  1,\n",
       "  26,\n",
       "  2,\n",
       "  27,\n",
       "  21,\n",
       "  9,\n",
       "  11,\n",
       "  34,\n",
       "  35,\n",
       "  2,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  33,\n",
       "  65,\n",
       "  28,\n",
       "  66,\n",
       "  22,\n",
       "  67,\n",
       "  31,\n",
       "  68,\n",
       "  22,\n",
       "  69,\n",
       "  70,\n",
       "  32,\n",
       "  71,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  10,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  30,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  10,\n",
       "  18,\n",
       "  11,\n",
       "  34,\n",
       "  35,\n",
       "  2,\n",
       "  8],\n",
       " [],\n",
       " [1,\n",
       "  83,\n",
       "  36,\n",
       "  21,\n",
       "  1,\n",
       "  36,\n",
       "  2,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  25,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  21,\n",
       "  9,\n",
       "  90,\n",
       "  10,\n",
       "  18,\n",
       "  13,\n",
       "  37,\n",
       "  12,\n",
       "  37,\n",
       "  19,\n",
       "  7,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  91,\n",
       "  17,\n",
       "  7,\n",
       "  3,\n",
       "  92,\n",
       "  93,\n",
       "  7,\n",
       "  94,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  3,\n",
       "  23,\n",
       "  22,\n",
       "  95,\n",
       "  96,\n",
       "  12,\n",
       "  14,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create tokenizer object and fit it on the corpus\n",
    "#Tokenizer object is used to convert words into numerical identifiers that can be processed by the model.\n",
    "tokenizer=Tokenizer()     \n",
    "tokenizer.fit_on_texts(corpus)#Tokenizer object learns the vocabulary of the corpus and creates a mapping between words and \n",
    "\t\t\t\t\t\t\t #their corresponding indices.\n",
    "\n",
    "\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "#The word_index dictionary stores the mapping between words and their corresponding indices\n",
    "#This dictionary is used to convert the text data into numerical data that can be processed by the model.\n",
    "\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(corpus)\n",
    "x,y=[],[]    #The x list will store the sequences aof word indices, and the y list will store the labels. The labels are 1 \n",
    "\t\t\t\t#for semantically similar pairs of words and 0 for dissimilar pairs.\n",
    "#generate sequences of word indices. \n",
    "#The sequences of word indices are generated using the tokenizer's texts_to_sequences method. \n",
    "#This method takes a list of text documents as input and returns a list of sequences of word indices. \n",
    "#Each sequence of word indices represents a single text document.\n",
    "\n",
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f3afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer Loop: The outer loop iterates through the sequence of words (seq), assigning each word to the variable target_word. \n",
    "\t\t\t#The enumerate() function is used to keep track of the index (i) of each word in the sequence.\n",
    "#Inner Loop: The inner loop iterates through a range of indices (j) around the current target word (target_word).\n",
    "\t\t\t#The starting index is max(0, i - 2), ensuring that it doesn't go below 0. \n",
    "    \t\t#The ending index is min(i + 3, len(seq)), ensuring that it doesn't exceed the sequence length.\n",
    "\n",
    "for seq in sequences:\n",
    "    for i, target_word in enumerate(seq): #enumerate() function is used in the code to generate sequences of positive and negative context words for each target word. Specifically, the code iterates through each sequence of word indices (seq) and assigns each word to the variable target_word. The enumerate() function is used to keep track of the index (i) of each word in the sequence.\n",
    "        for j in range(max(0,i-2),min(i+3,len(seq))):\n",
    "            if i != j:\n",
    "                x.append([target_word, seq[j]])\n",
    "                y.append(1)\n",
    "                x.append([target_word,np.random.choice(list(word_index.values()))])\n",
    "                y.append(0)\n",
    "#The provided code snippet iterates through each sequence of words (seq) in the sequences list.\n",
    "#For each target word (target_word) in the current sequence, it extracts two context words: one positive context word \n",
    "\t#(seq[j]) and one negative context word (np.random.choice(list(word_index.values()))).               \n",
    "#Positive context word- semantically same to target word\n",
    "#Negative context word- not semantically similar to the target word.(choosen randomly from the vocabulary)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#convert the lists of word indices (x) and labels (y) to NumPy arrays for efficient processing.\n",
    "#NumPy arrays are more efficient to use than lists because they can be manipulated more quickly and easily.\n",
    "x=np.array(x)                \n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c17bb5",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c213016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2, 1)              103       \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 1)                 0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105 (420.00 Byte)\n",
      "Trainable params: 105 (420.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=1, input_length=2),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#Embedding: This layer embeds each word pair into a low-dimensional vector. The input_dim parameter specifies the vocabulary\n",
    "#size (the number of unique words), and the output_dim parameter determines the embedding dimension. \n",
    "#The embedding dimension is a hyperparameter that can be tuned to improve the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "#GlobalAveragePooling1D: This layer averages the embedding vectors for the two words in each pair. This means that the\n",
    "\t#output of this layer is a single vector that represents the semantic similarity of the two words.\n",
    "#always used with Embeddinglayer \n",
    "\n",
    "\n",
    "#Dense: This layer outputs a single value representing the predicted probability of the two words being semantically \n",
    "#similar. The activation function is sigmoid, which produces a value between 0 and 1. A value of 1 means that the model \n",
    "#predicts that the two words are semantically similar, and a value of 0 means that the model predicts that the two words \n",
    "#are not semantically similar.\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8adbf3",
   "metadata": {},
   "source": [
    "## Model COmpilation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aedd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.4930\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5242\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5458\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5662\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5814\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5935\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.6081\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5929\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.6018\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5929\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.5935\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5910\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5910\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6841 - accuracy: 0.5891\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5941\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6818 - accuracy: 0.5935\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.5980\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6795 - accuracy: 0.5954\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6785 - accuracy: 0.5954\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6773 - accuracy: 0.5941\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5992\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.5992\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6744 - accuracy: 0.5973\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6736 - accuracy: 0.5954\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6726 - accuracy: 0.5967\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6717 - accuracy: 0.6018\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.6037\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.6011\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6695 - accuracy: 0.6018\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.6011\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6684 - accuracy: 0.6024\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.6069\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6673 - accuracy: 0.6069\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.6113\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.6050\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.6043\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.6069\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.6101\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.6081\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.6075\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.6107\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.6088\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.6101\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.6126\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6633 - accuracy: 0.6120\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.6126\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6630 - accuracy: 0.6120\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.6126\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.6139\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.6177\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.6170\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.6190\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6145\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6126\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.6145\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6139\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6151\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6164\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.6132\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.6151\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6151\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6120\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6613 - accuracy: 0.6126\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6113\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6132\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6120\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6107\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6610 - accuracy: 0.6107\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6609 - accuracy: 0.6107\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6094\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6609 - accuracy: 0.6126\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6113\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.6139\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6158\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6132\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6164\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.6145\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6145\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6158\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6151\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6164\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6151\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.6164\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6606 - accuracy: 0.6158\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.6170\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6164\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.6170\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6158\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.6151\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6190\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6196\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6164\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6177\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6177\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6158\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6183\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.6183\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6158\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6145\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6158\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "H=model.fit(x,y,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f988cf0",
   "metadata": {},
   "source": [
    "## Word Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b317cb45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: [-0.50820553]\n",
      "of: [-0.39724612]\n",
      "influenza: [-0.36985838]\n",
      "covid: [-0.39427823]\n",
      "19: [-0.25514337]\n",
      "virus: [-0.33730212]\n",
      "for: [-0.4163628]\n",
      "transmission: [-0.25232625]\n",
      "is: [-0.33559835]\n",
      "to: [-0.15449837]\n",
      "a: [-0.3972882]\n",
      "and: [-0.34976885]\n",
      "between: [-0.18421139]\n",
      "time: [-0.42584416]\n",
      "serial: [-0.04274696]\n",
      "interval: [-0.2568933]\n",
      "than: [-0.2349225]\n",
      "be: [0.0002841]\n",
      "5: [-0.26301587]\n",
      "days: [-0.05872761]\n",
      "â€“: [-0.22338726]\n",
      "are: [-0.46534672]\n",
      "viruses: [0.01737609]\n",
      "shorter: [-0.08138589]\n",
      "from: [-0.4590055]\n",
      "appearance: [0.03403454]\n",
      "symptoms: [0.05540299]\n",
      "while: [-0.10184057]\n",
      "3: [-0.00510487]\n",
      "this: [-0.49724892]\n",
      "that: [0.22803833]\n",
      "can: [-0.37410295]\n",
      "in: [-0.08756864]\n",
      "major: [-0.21287668]\n",
      "driver: [0.01566189]\n",
      "number: [0.14530027]\n",
      "2: [-0.00539199]\n",
      "speed: [0.38281143]\n",
      "an: [-0.02985298]\n",
      "important: [-0.02705348]\n",
      "point: [-0.09553389]\n",
      "difference: [0.2614949]\n",
      "two: [0.15722242]\n",
      "has: [0.06253094]\n",
      "median: [0.02398031]\n",
      "incubation: [0.08154372]\n",
      "period: [0.04801432]\n",
      "infection: [0.21617281]\n",
      "successive: [0.3703797]\n",
      "cases: [0.6058912]\n",
      "estimated: [0.01310583]\n",
      "6: [0.2545989]\n",
      "means: [0.6086411]\n",
      "spread: [0.35019812]\n",
      "faster: [0.42554352]\n",
      "further: [-0.03948794]\n",
      "first: [0.53581715]\n",
      "illness: [0.38475236]\n",
      "or: [0.14650157]\n",
      "potentially: [0.10511112]\n",
      "pre: [0.35286418]\n",
      "symptomatic: [0.46295062]\n",
      "â€“transmission: [0.24844517]\n",
      "before: [0.06391142]\n",
      "contrast: [0.15407468]\n",
      "we: [0.48115942]\n",
      "learning: [0.33552098]\n",
      "there: [-0.0188163]\n",
      "people: [0.33734822]\n",
      "who: [0.01714113]\n",
      "shed: [0.21697348]\n",
      "24: [0.29231456]\n",
      "48: [0.06292924]\n",
      "hours: [0.2490798]\n",
      "prior: [-0.3261455]\n",
      "symptom: [0.28532013]\n",
      "onset: [0.47652298]\n",
      "at: [0.31357363]\n",
      "present: [0.24221149]\n",
      "does: [0.20686086]\n",
      "not: [0.20427066]\n",
      "appear: [0.36270902]\n",
      "reproductive: [0.48709252]\n",
      "secondary: [0.5353591]\n",
      "infections: [0.54622334]\n",
      "generated: [0.18859573]\n",
      "one: [-0.09494913]\n",
      "infected: [0.22142701]\n",
      "individual: [0.3579806]\n",
      "understood: [0.2921664]\n",
      "higher: [0.4268496]\n",
      "however: [-0.00457663]\n",
      "estimates: [0.4094454]\n",
      "both: [0.31816223]\n",
      "very: [0.37838203]\n",
      "context: [0.32233578]\n",
      "specific: [0.28047892]\n",
      "making: [0.11595067]\n",
      "direct: [-0.04483335]\n",
      "comparisons: [0.29632214]\n",
      "more: [0.0337741]\n",
      "difficult: [0.38301438]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings=model.layers[0].get_weights()[0]#extracts the weights of the embedding layer.\n",
    "#embedding layer is the first layer in the model, it's accessed with index 0\n",
    "#get_weigths() method returns a list of weights for the layer & the first element of the list corresponds to the embedding \n",
    "\n",
    "for word, index in word_index.items():   #word_index dictionary is used to map each word to its corresponding index in\n",
    "    \t\t\t\t\t\t\t\t\t #the embedding matrix.\n",
    "    print(f\"{word}: {word_embeddings[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e20eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Prediction\n",
    "word_to_predict = \"period\"\n",
    "context_word = tokenizer.texts_to_sequences([[word_to_predict]])[0][0]\n",
    "#selecct a traget word and a random context word\n",
    "\n",
    "#convert it into vector\n",
    "context_vector = np.array([context_word, np.random.choice(list(word_index.values()))])\n",
    "\n",
    "predicted_proba = model.predict(np.array([context_vector]))\n",
    "print(f\"Predicted probability for context '{word_to_predict}': {predicted_proba[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd689fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Plot graph\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(H.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
